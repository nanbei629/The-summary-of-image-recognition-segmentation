# 深度学习模型简介——图像分类



![robot6](img\robot6.png)

以下是我在毕设过程中接触到的网络，除此之外还有 **LeNet-5**  **AlexNet**  (比较早期的网络)，**GoogleNet**

### CNN

![cnn](img\cnn.jpg)

**基础知识**

* 卷积
* 池化 （pooling）
* 激励函数（activation function）
* 全连接层
* 反向传播 

这些概念是了解卷积神经网络的基础，大家可以在后面资料分享提及的课程中进行学习。

### VGG

VGGNet是Karen Simonyan和 Andrew Zisserman在2014年提出的深度神经网络架构，在ImageNet中取得定位任务第一名和分类任务第二名的成绩。VGGNet增加了网络深度减小了卷积核的大小，使得网络更加的有效。

**特点**：

* 更加深
* 图像预处理只是将像素值减去平均值
* 用到的卷积核尺寸均为3*3
* 卷积核后跟3个全连接层
* 激活函数为RELU

[论文链接](http://xueshu.baidu.com/s?wd=paperuri%3A%282801f41808e377a1897a3887b6758c59%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Farxiv.org%2Fpdf%2F1409.1556&ie=utf-8&sc_us=7831563276134646431)

### ResNet

深度残差网络的目的是高效的训练一个非常深层的网络，用residual block来解决训练深层网络过程中梯度消失的问题。

**Residual block**

![rest](img\rest.PNG)

Rest训练的网络是VGG的8倍

[论文链接](http://xueshu.baidu.com/s?wd=paperuri%3A%283821a90f58762386e257eb4e6fa11f79%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Farxiv.org%2Fabs%2F1512.03385&ie=utf-8&sc_us=8691480010698282026)

[视频教学](https://www.youtube.com/watch?v=K0uoBKBQ1gA)

### 模型微调（fine tuning）

当数据量不大时，我们大多是利用别人的模型和参数，在此基础上进行微调，这其实属于迁移学习范畴。

![trasnferlearning](img\trasnferlearning.PNG)



**在深度学习领域为什么可以进行微调？**

因为在深度学习网络模型中，浅层网络学习的是最基础的特征，如:颜色，有无直线，几何图形，深层网络学习的则是独有的特征。

所以我们可以利用别人的网络来训练自己的数据，保持前几层网络的参数不变，只训练深层网络的参数，这样不仅加快了训练速度，也解决了数据量小的问题

**相关资料**

1、[《How transferable are features in deep neural  networks?》](http://xueshu.baidu.com/s?wd=paperuri%3A%288893c347d545d6edd2cb2fede1c3dfa6%29&filter=sc_long_sign&tn=SE_xueshusource_2kduw22v&sc_vurl=http%3A%2F%2Farxiv.org%2Fabs%2F1411.1792&ie=utf-8&sc_us=7158072134074889645)

像前面我们提到的，神经网络的浅层训练的特征具有普遍性（general），深层训练的特征具有特殊性（specific）。该篇论文通过做大量的实验为了回答三个问题

- 我们如何判断那一层是general，那一层是specific
- 从general层到specific层的转换是突然发生在一个单一的层，还是分散在多个层
- 这种转换发生在哪里

可以帮助我们微调网络

2、[知乎专栏文章](https://zhuanlan.zhihu.com/p/22624331)

3、[Must Know Tips/Tricks in Deep Neural Networks]([http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html))







